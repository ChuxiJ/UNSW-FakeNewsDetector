{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install git+https://www.github.com/bojone/bert4keras.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install wget","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport json\nimport numpy as np\nimport pandas as pd\nfrom random import choice\nimport re, os\nimport codecs\nfrom bert4keras.bert import load_pretrained_model, set_gelu\nfrom bert4keras.train import PiecewiseLinearLearningRate\nfrom bert4keras import *\nfrom bert4keras.backend import *\nfrom fastai.text import *\nfrom collections import OrderedDict, defaultdict\nimport wget\n\nimport zipfile\n\nfrom keras.layers import *\nfrom keras.models import Model\nimport keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import KFold\n\nfrom transformers import *\n\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls ../input/preprocessed-fake-news","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataLoader():\n    \n    def __init__(self, df, column=\"text\", batch_size=8, train=True):\n        \n        self.batch_size = batch_size\n        \n        if train:\n            df = df[[\"id\", \"label\", column]].sample(frac=1., random_state=42).reset_index(drop=True)\n        else:\n            df = df[[\"id\", column]].sample(frac=1., random_state=42).reset_index(drop=True)\n        \n        df['ids'] = df[column].map(self.get_ids)\n        self.df = df.rename(columns={column:\"text\"})\n        \n        if train:\n            self.train_step, self.valid_step = len(df)*0.8//batch_size, len(df)*0.2//batch_size\n            self.train_step += len(df)*0.8%batch_size!=0\n            self.valid_step += len(df)*0.2%batch_size!=0\n            \n            kf = KFold(5, True, 42)\n\n            fake_df, true_df = df[df.label==1], df[df.label==0]\n\n            fake_train_folders, fake_valid_folders = [], []\n            for train_index, valid_index in kf.split(fake_df):\n                fake_train_folders.append(fake_df.iloc[train_index])\n                fake_valid_folders.append(fake_df.iloc[valid_index])\n\n            true_train_folders, true_valid_folders = [], []\n            for train_index, valid_index in kf.split(true_df):\n                true_train_folders.append(true_df.iloc[train_index])\n                true_valid_folders.append(true_df.iloc[valid_index])\n\n            folders = []\n            for i in range(5):\n                train_df = fake_train_folders[i].append(true_train_folders[i], ignore_index=True).sample(frac=1)\n                valid_df = fake_valid_folders[i].append(true_valid_folders[i], ignore_index=True).sample(frac=1)\n                folders.append((train_df, valid_df))\n            self.folders = folders\n    \n    def get_ids(self, text):\n        return [101] + tokenizer.encode(text)[:383]\n        \n    def __getitem__(self, i):\n        train_df, valid_df = self.folders[i]\n        return self.data_generator(train_df), self.data_generator(valid_df)\n    \n    def data_generator(self, data):\n        while True:\n            for i, g in data.groupby(np.arange(len(data))//self.batch_size):\n                ids = pad_sequences(g.ids.values, padding='post')\n                segment_ids = np.zeros((ids.shape), dtype=np.uint32)\n                yield [ids, segment_ids], g.label.values\n            \n    def get_prediction(self, model, col_name):\n        new_df = defaultdict(list)\n        for i, e in self.df.iterrows():\n            new_df['id'].append(e.id)\n            ids = np.array(e.ids)[None]\n            segment_ids = np.zeros((ids.shape), dtype=np.uint32)\n            y_pred = model.predict([ids, segment_ids])\n            new_df[col_name].append(y_pred[0][0])\n        return pd.DataFrame(new_df).sort_values('id')\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef build_model(name='albert_base', keep_words=None):\n    \n    set_gelu('tanh') # 切换gelu版本\n\n    albert_links =  OrderedDict(albert_xlarge = \"https://storage.googleapis.com/albert_zh/albert_xlarge_zh.zip\",\n                    albert_large = \"https://storage.googleapis.com/albert_zh/albert_large_zh.zip\",\n                    albert_base  = \"https://storage.googleapis.com/albert_zh/albert_base_zh.zip\",\n                    vocab = \"https://github.com/brightmart/albert_zh/blob/master/albert_config/vocab.txt\")\n    config_paths = OrderedDict(albert_xlarge= \"./albert_xlarge/albert_config_xlarge.json\",\n                               albert_large = \"./albert_large/albert_config_large.json\",\n                               albert_base = \"./albert_base/albert_config_base.json\")\n    \n    if not os.path.isdir(name):\n        os.mkdir(name)\n        wget.download(albert_links[name], name+'.zip')\n        with zipfile.ZipFile(name+\".zip\", 'r') as zip_ref:\n            zip_ref.extractall(name)\n            os.remove(name+'.zip')\n\n    model = load_pretrained_model(\n        config_paths[name],\n        name+'/albert_model.ckpt',\n        keep_words=keep_words,\n        albert=True\n    )\n\n    output = Lambda(lambda x: x[:, 0])(model.output)\n    output = Dense(1, activation='sigmoid')(output)\n    model = Model(model.input, output)\n\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=Adam(1e-5),\n        metrics=['accuracy']\n    )\n    # model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = Path(\"../input/preprocessed-fake-news\")\ntrain_df = pd.read_csv(data_path/\"train_unstructed_remove_number_data.csv\", encoding=\"utf8\")\ntest_df = pd.read_csv(data_path/\"test_unstructed_remove_number_data.csv\", encoding=\"utf8\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chinese_text_columns = ['text', 'pre_20_words', 'first_sentence', 'last_centence', 're_translate']\n\n# 'albert_xlarge' is too large\nmodels = ['albert_large', 'albert_base']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_loader = DataLoader(train_df)\ntest_data_loader  = DataLoader(test_df, train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_result = None\ntest_result = None\n\nfor model_name in models[1:2]:\n    \n    model = build_model(model_name)\n    \n    for i in range(5):\n        \n        train_g, valid_g = train_data_loader[i]\n        \n        model.fit_generator(\n            train_g,\n            steps_per_epoch=train_data_loader.train_step,\n            epochs=2,\n            validation_data=valid_g,\n            validation_steps=train_data_loader.valid_step\n        )\n        \n        train_predictions = train_data_loader.get_prediction(model, f\"{model_name}_{i}\")\n        if train_result:\n            train_result = pd.concat([train_result, train_predictions], axis=1).drop('id', axis=1)\n        else:\n            train_result = train_predictions\n        \n        test_predictions = test_data_loader.get_prediction(model, f\"{model_name}_{i}\")\n        if test_result:\n            test_result = pd.concat([test_result, test_predictions], axis=1).drop('id', axis=1)\n        else:\n            test_result = test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_result.to_csv(\"train_result.csv\", index=False)\ntest_result.to_csv(\"test_result.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}